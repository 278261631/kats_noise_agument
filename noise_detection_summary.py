#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å™ªç‚¹æ£€æµ‹ç»“æœæ€»ç»“
å¯¹æ¯”ä¸åŒæ–¹æ³•çš„æ£€æµ‹æ•ˆæœ
"""

import numpy as np
from astropy.io import fits
import matplotlib.pyplot as plt
import os

def analyze_fits_file(filename):
    """åˆ†æFITSæ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯"""
    
    if not os.path.exists(filename):
        return None
    
    with fits.open(filename) as hdul:
        data = hdul[0].data.astype(np.float64)
        
        stats = {
            'filename': filename,
            'shape': data.shape,
            'mean': np.mean(data),
            'std': np.std(data),
            'min': np.min(data),
            'max': np.max(data),
            'non_zero_pixels': np.sum(data != 0),
            'total_pixels': data.size
        }
        
        return stats

def compare_noise_detection_results():
    """å¯¹æ¯”ä¸åŒå™ªç‚¹æ£€æµ‹æ–¹æ³•çš„ç»“æœ"""
    
    print("ğŸ” å™ªç‚¹æ£€æµ‹ç»“æœå¯¹æ¯”åˆ†æ")
    print("=" * 60)
    
    # æŸ¥æ‰¾åŸå§‹æ–‡ä»¶
    fits_files = [f for f in os.listdir('.') if f.endswith('.fit')]
    if not fits_files:
        print("âŒ æœªæ‰¾åˆ°åŸå§‹FITSæ–‡ä»¶")
        return
    
    original_file = fits_files[0]
    base_name = os.path.splitext(original_file)[0]
    
    print(f"ğŸ“ åŸå§‹æ–‡ä»¶: {original_file}")
    
    # åˆ†æåŸå§‹å›¾åƒ
    original_stats = analyze_fits_file(original_file)
    if original_stats:
        print(f"\nğŸ“Š åŸå§‹å›¾åƒç»Ÿè®¡:")
        print(f"  å°ºå¯¸: {original_stats['shape']}")
        print(f"  æ€»åƒç´ æ•°: {original_stats['total_pixels']:,}")
        print(f"  å‡å€¼: {original_stats['mean']:.2f}")
        print(f"  æ ‡å‡†å·®: {original_stats['std']:.2f}")
        print(f"  æ•°æ®èŒƒå›´: [{original_stats['min']:.0f}, {original_stats['max']:.0f}]")
    
    # å®šä¹‰è¦åˆ†æçš„æ–‡ä»¶ç±»å‹
    analysis_files = [
        # å°æ³¢å»å™ªç»“æœ
        (f"{base_name}_denoised.fits", "æ ‡å‡†å°æ³¢å»å™ª", "å»å™ªå›¾åƒ"),
        (f"{base_name}_noise.fits", "æ ‡å‡†å°æ³¢å™ªå£°", "å™ªå£°æå–"),
        (f"{base_name}_sharp_denoised.fits", "é”åˆ©å°æ³¢å»å™ª", "å»å™ªå›¾åƒ"),
        (f"{base_name}_sharp_noise.fits", "é”åˆ©å°æ³¢å™ªå£°", "å™ªå£°æå–"),
        (f"{base_name}_ultra_sharp_denoised.fits", "è¶…é”åˆ©å°æ³¢å»å™ª", "å»å™ªå›¾åƒ"),
        (f"{base_name}_ultra_sharp_noise.fits", "è¶…é”åˆ©å°æ³¢å™ªå£°", "å™ªå£°æå–"),
        
        # å•åƒç´ æ£€æµ‹ç»“æœ
        (f"{base_name}_simple_repaired.fits", "å•åƒç´ ä¿®å¤", "å»å™ªå›¾åƒ"),
        (f"{base_name}_simple_noise.fits", "å•åƒç´ å™ªå£°", "å™ªå£°æå–"),
        (f"{base_name}_hot_pixels_simple.fits", "çƒ­åƒç´ æ£€æµ‹", "å™ªå£°æå–"),
        (f"{base_name}_cold_pixels_simple.fits", "å†·åƒç´ æ£€æµ‹", "å™ªå£°æå–"),
    ]
    
    print(f"\nğŸ”¬ å„ç§æ–¹æ³•æ£€æµ‹ç»“æœ:")
    print("-" * 80)
    print(f"{'æ–¹æ³•':<20} {'ç±»å‹':<12} {'å™ªç‚¹æ•°é‡':<12} {'å™ªç‚¹å æ¯”':<12} {'å™ªå£°å¼ºåº¦':<12}")
    print("-" * 80)
    
    results = []
    
    for filename, method_name, file_type in analysis_files:
        stats = analyze_fits_file(filename)
        if stats:
            if file_type == "å™ªå£°æå–":
                noise_count = stats['non_zero_pixels']
                noise_ratio = (noise_count / stats['total_pixels']) * 100
                noise_intensity = stats['std'] if noise_count > 0 else 0
                
                print(f"{method_name:<20} {file_type:<12} {noise_count:<12,} {noise_ratio:<12.6f}% {noise_intensity:<12.2f}")
                
                results.append({
                    'method': method_name,
                    'type': file_type,
                    'noise_count': noise_count,
                    'noise_ratio': noise_ratio,
                    'noise_intensity': noise_intensity
                })
            else:
                # å¯¹äºå»å™ªå›¾åƒï¼Œè®¡ç®—ä¸åŸå§‹å›¾åƒçš„å·®å¼‚
                if original_stats:
                    mean_diff = abs(stats['mean'] - original_stats['mean'])
                    std_diff = abs(stats['std'] - original_stats['std'])
                    
                    print(f"{method_name:<20} {file_type:<12} {'N/A':<12} {'N/A':<12} {std_diff:<12.2f}")
    
    # åˆ†æå•åƒç´ æ£€æµ‹çš„è¯¦ç»†ç»“æœ
    print(f"\nğŸ¯ å•åƒç´ å™ªç‚¹æ£€æµ‹è¯¦ç»†åˆ†æ:")
    print("-" * 50)
    
    # çƒ­åƒç´ åˆ†æ
    hot_stats = analyze_fits_file(f"{base_name}_hot_pixels_simple.fits")
    cold_stats = analyze_fits_file(f"{base_name}_cold_pixels_simple.fits")
    
    if hot_stats and cold_stats:
        hot_count = hot_stats['non_zero_pixels']
        cold_count = cold_stats['non_zero_pixels']
        total_single_pixels = hot_count + cold_count
        
        print(f"çƒ­åƒç´ æ•°é‡: {hot_count:,}")
        print(f"å†·åƒç´ æ•°é‡: {cold_count:,}")
        print(f"å•åƒç´ å™ªç‚¹æ€»æ•°: {total_single_pixels:,}")
        print(f"å•åƒç´ å™ªç‚¹å æ¯”: {(total_single_pixels / original_stats['total_pixels']) * 100:.6f}%")
        
        if hot_count > 0:
            with fits.open(f"{base_name}_hot_pixels_simple.fits") as hdul:
                hot_data = hdul[0].data
                hot_values = hot_data[hot_data > 0]
                print(f"çƒ­åƒç´ å€¼èŒƒå›´: [{np.min(hot_values):.0f}, {np.max(hot_values):.0f}]")
                print(f"çƒ­åƒç´ å¹³å‡å€¼: {np.mean(hot_values):.2f}")
        
        if cold_count > 0:
            with fits.open(f"{base_name}_cold_pixels_simple.fits") as hdul:
                cold_data = hdul[0].data
                cold_values = cold_data[cold_data > 0]
                print(f"å†·åƒç´ å€¼èŒƒå›´: [{np.min(cold_values):.0f}, {np.max(cold_values):.0f}]")
                print(f"å†·åƒç´ å¹³å‡å€¼: {np.mean(cold_values):.2f}")
    
    # æ–¹æ³•å¯¹æ¯”æ€»ç»“
    print(f"\nğŸ“ˆ æ–¹æ³•å¯¹æ¯”æ€»ç»“:")
    print("-" * 50)
    print("1. å°æ³¢å»å™ªæ–¹æ³•:")
    print("   - æ ‡å‡†å°æ³¢: é€‚åˆä¸€èˆ¬å™ªå£°å»é™¤")
    print("   - é”åˆ©å°æ³¢: é’ˆå¯¹å°é¢ç§¯é”åˆ©å™ªç‚¹ä¼˜åŒ–")
    print("   - è¶…é”åˆ©å°æ³¢: æç«¯æ•æ„Ÿåº¦ï¼Œæ•è·æœ€ç»†å¾®å™ªç‚¹")
    print()
    print("2. å•åƒç´ æ£€æµ‹æ–¹æ³•:")
    print("   - çƒ­åƒç´ æ£€æµ‹: æ£€æµ‹å¼‚å¸¸äº®çš„å•åƒç´ ")
    print("   - å†·åƒç´ æ£€æµ‹: æ£€æµ‹å¼‚å¸¸æš—çš„å•åƒç´ ")
    print("   - ä¿®å¤æ•ˆæœ: ä¿æŒå›¾åƒæ•´ä½“ç‰¹å¾çš„åŒæ—¶å»é™¤å­¤ç«‹å™ªç‚¹")
    
    # æ¨èä½¿ç”¨åœºæ™¯
    print(f"\nğŸ’¡ æ¨èä½¿ç”¨åœºæ™¯:")
    print("-" * 50)
    print("â€¢ å¤©æ–‡å›¾åƒå¤„ç†: ä½¿ç”¨å•åƒç´ æ£€æµ‹å»é™¤çƒ­åƒç´ å’Œå†·åƒç´ ")
    print("â€¢ ä¸€èˆ¬å»å™ª: ä½¿ç”¨æ ‡å‡†å°æ³¢å»å™ª")
    print("â€¢ ç²¾ç»†å¤„ç†: ä½¿ç”¨é”åˆ©å°æ³¢å»å™ª")
    print("â€¢ æç«¯æƒ…å†µ: ä½¿ç”¨è¶…é”åˆ©å°æ³¢å»å™ª")
    
    return results

def create_detection_report():
    """åˆ›å»ºæ£€æµ‹æŠ¥å‘Š"""
    
    print("\nğŸ“‹ ç”Ÿæˆæ£€æµ‹æŠ¥å‘Š...")
    
    results = compare_noise_detection_results()
    
    # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
    report_filename = "noise_detection_report.txt"
    
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write("å™ªç‚¹æ£€æµ‹åˆ†ææŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")
        
        f.write("å¤„ç†çš„æ–‡ä»¶:\n")
        fits_files = [f for f in os.listdir('.') if f.endswith('.fit')]
        if fits_files:
            f.write(f"åŸå§‹æ–‡ä»¶: {fits_files[0]}\n\n")
        
        f.write("ç”Ÿæˆçš„å¤„ç†ç»“æœæ–‡ä»¶:\n")
        result_files = [f for f in os.listdir('.') if f.endswith('.fits')]
        for file in sorted(result_files):
            f.write(f"- {file}\n")
        
        f.write(f"\næ£€æµ‹åˆ°çš„æ–‡ä»¶æ€»æ•°: {len(result_files)}\n")
        
        f.write("\nå¤„ç†æ–¹æ³•è¯´æ˜:\n")
        f.write("1. å°æ³¢é˜ˆå€¼å»å™ª: ä½¿ç”¨å°æ³¢å˜æ¢å»é™¤å™ªå£°\n")
        f.write("2. å•åƒç´ æ£€æµ‹: ä¸“é—¨æ£€æµ‹å’Œä¿®å¤å•ä¸ªåƒç´ çš„å¼‚å¸¸å€¼\n")
        f.write("3. çƒ­åƒç´ æ£€æµ‹: æ£€æµ‹å¼‚å¸¸äº®çš„åƒç´ \n")
        f.write("4. å†·åƒç´ æ£€æµ‹: æ£€æµ‹å¼‚å¸¸æš—çš„åƒç´ \n")
    
    print(f"ğŸ“„ æ£€æµ‹æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filename}")

def main():
    try:
        create_detection_report()
        print(f"\nâœ… åˆ†æå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
